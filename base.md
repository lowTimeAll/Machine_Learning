# 基础

* 损失函数：一个样本的误差。
* 代价函数：整个训练样本的误差总和。
* 经验风险：模型 *&fnof;(x)* 关于训练数据集上的平均损失。
* 期望损失(风险损失)：模型 *&fnof;(x)* 关于联合分布 *P(X,Y)* 的平均意义下的损失，存在一个问题联合分布未知，无法求解。
* 目标函数：代价函数 + 正则化项(防止过拟合,代价函数并不是越小越好，会产生过拟合，所以引入了正则化措施)。

* 偏差：指预测输出与真实标签的差别，记作， *bias<sup>2</sup>(X) = (&fnof;(X) - Y)<sup>2</sup>*
        度量了算法的期望预测与真实结果的偏离程度，刻画了算法本身的拟合能力。
* 方差：在一个训练集上训练得到的模型，与所有训练数据得到的平均函数的差的平方再取期望，记作， *Var(X) = E[(&fnof;(X;D<sub>i</sub>) - &fnof;(x))<sup>2</sup>]*
        (比如，存在数据集D，将其划分为10份，对于每一个数据集训练一个模型，上式中的第二项即为是个模型预测的平均值，第一项为单个模型)，度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据
        扰动所造成的影响。
* 欠拟合：模型的训练集损失函数较大，验证集损失函数也较大。模型不够复杂或者训练数据较少时，会出现偏差，准确率下降，称为欠拟合。
  *  添加其它特征项，模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征来更好地解决；
  *  添加多项式特征。例如将线性模型通过添加二次项或者三次想使模型的泛化能力更强；
  *  减少正则化参数。
* 过拟合：模型的训练集损失函数很小，验证集损失函数较大。模型过于复杂或者没有足够的数据支持模型的训练，模型含有训练集的特定信息，对训练集过于依赖，即模型对于训练集高度敏感，称为过拟合。
  *  重新清洗数据，导致过拟合的一个原因可能是数据含有噪声导致的；
  *  增大数据的训练量；
  *  使用正则化方法；
  *  神经网络：提前停止、交叉验证、dropout
* 学习曲线：描述模型的损失函数随样本数量变化的曲线。
            模型欠拟合时，训练集和验证集的学习曲线很接近，但都在一个较低的水平。模型过拟合时，训练集的学习曲线保持在高水平，验证集的学习曲线保持在较低的水平。模型拟合很好时，训练集和验证集的学习曲线接近，且都保持在较高水平。

    ![偏差-方差曲线](./src/image/bias-var.jpg)

* 梯度消失和梯度爆炸：训练很深的网络时，随着层数的增加，导数会出现指数级的下降，则导致梯度消失，或者指数级的增加，导致梯度爆炸，本质时梯度传递的链式法则导致矩阵高次幂（反向传播会逐层对函数求偏导相乘）。
*  梯度消失：网络层之间的梯度值小于1.0重复相乘导致的指数级减小会产生梯度消失，主要是网络层数太深，导致梯度无法传播，本质是激活函数的饱和性。
    *   解决办法：
        *    合理初始化权重值，初始化权重时，使每个神经元尽可能不要取极大值二号极小值，躲开梯度消失的区域；
        *    使用relu代替sigmoid和tanh作为激活函数；
        *    使用其他网络结构，例如LSTM和GRU。
*  梯度爆炸：网络层之间的梯度值大于1.0重复相乘导致的指数级增长会导致梯度爆炸。梯度爆炸就是由于初始化权重过大，前面层比后面层变化的更快，就会导致权值越来越大，主要是误差积累。
    *   解决办法：
        *    可设置阈值进行梯度修剪；
        *    权值正则化。

* 常见的初始化方式
    *  标准初始化：标准均匀初始化，标准正态分布
    *  Xavier初始化：Xavier均匀分布初始化，Xavier正态初始化
    *  He初始化：He均匀分布初始化，He正态分布初始化